{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_splits = 11\n",
    "results_folder = \"../../../results/similarity_boosted_omega_0_5/distinction/hybrid_rbm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_auc_0 = []\n",
    "average_precision_0 = []\n",
    "mcc_arr_0 = []\n",
    "interaction_0 = \"y_true_test_direct\"\n",
    "prob_0 = \"prob_test_direct\"\n",
    "pred_0 = \"y_prediction_test_direct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, n_splits):\n",
    "    y_true_file = open(\"{0}{1}/{2}.csv\".format(results_folder, i, interaction_0),\"r\")\n",
    "    prob_file = open(\"{0}{1}/{2}.csv\".format(results_folder, i, prob_0),\"r\")\n",
    "    pred_file = open(\"{0}{1}/{2}.csv\".format(results_folder, i, pred_0),\"r\")\n",
    "    \n",
    "    y_true = np.array([float(val) for val in y_true_file.read().split(',')])\n",
    "    y_score = np.array([float(val) for val in prob_file.read().split(',')])\n",
    "    y_pred = np.array([float(val) for val in pred_file.read().split(',')])\n",
    "    \n",
    "    y_true_file.close()\n",
    "    pred_file.close()\n",
    "    prob_file.close()\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    roc_auc_0.append(auc(fpr, tpr))\n",
    "\n",
    "    plt.title('AUC')\n",
    "\n",
    "    plt.plot(fpr, tpr, color='red', lw=1, label='ROC curve (area = %0.2f)' % auc(fpr, tpr))\n",
    "    plt.xlabel('False Positive Rate (1- specificity)')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    average_precision_0.append(average_precision_score(y_true, y_score))\n",
    "\n",
    "    plt.title('AUPR')\n",
    "\n",
    "    plt.plot(recall, precision, color='red', lw=1, label='ROC curve (area = %0.2f)' % average_precision_score(y_true, y_score))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    mcc_arr_0.append(matthews_corrcoef(y_true, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_auc_1 = []\n",
    "average_precision_1 = []\n",
    "mcc_arr_1 = []\n",
    "interaction_1 = \"y_true_test_indirect\"\n",
    "prob_1 = \"prob_test_indirect\"\n",
    "pred_1 = \"y_prediction_test_indirect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, n_splits):\n",
    "    y_true_file = open(\"{0}{1}/{2}.csv\".format(results_folder, i, interaction_1),\"r\")\n",
    "    prob_file = open(\"{0}{1}/{2}.csv\".format(results_folder, i, prob_1),\"r\")\n",
    "    pred_file = open(\"{0}{1}/{2}.csv\".format(results_folder, i, pred_1),\"r\")\n",
    "    \n",
    "    y_true = np.array([float(val) for val in y_true_file.read().split(',')])\n",
    "    y_pred = np.array([float(val) for val in pred_file.read().split(',')])\n",
    "    y_score = np.array([float(val) for val in prob_file.read().split(',')])\n",
    "    \n",
    "    y_true_file.close()\n",
    "    pred_file.close()\n",
    "    prob_file.close()\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    roc_auc_1.append(auc(fpr, tpr))\n",
    "\n",
    "    plt.title('AUC')\n",
    "\n",
    "    plt.plot(fpr, tpr, color='red', lw=1, label='ROC curve (area = %0.2f)' % auc(fpr, tpr))\n",
    "    plt.xlabel('False Positive Rate (1- specificity)')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    average_precision_1.append(average_precision_score(y_true, y_score))\n",
    "\n",
    "    plt.title('AUPR')\n",
    "\n",
    "    plt.plot(recall, precision, color='red', lw=1, label='ROC curve (area = %0.2f)' % average_precision_score(y_true, y_score))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    mcc_arr_1.append(matthews_corrcoef(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Results analysis for direct interaction')\n",
    "print('AUC: {0:0.2f} \\nAUPR: {1:0.2f} \\nNum: {2} \\nMCC: {3:0.2f}'.format(np.mean(roc_auc_0)*100, np.mean(average_precision_0)*100, len(average_precision_0), np.mean(mcc_arr_0)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Results analysis for indirect interaction')\n",
    "print('AUC: {0:0.2f} \\nAUPR: {1:0.2f} \\nNum: {2} \\nMCC: {3:0.2f}'.format(np.mean(roc_auc_1)*100, np.mean(average_precision_1)*100, len(average_precision_1), np.mean(mcc_arr_1)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
